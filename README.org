# -*- ispell-dictionary: "english" -*-


#+AUTHOR: David √Ålvarez Rosa
#+EMAIL: david@alvarezrosa.com
#+STARTUP: showall


* Neural Network
Implementation of a feedforward (fully connected) Neural Network from scratch
in C++, based on the object-oriented programming paradigm. Accordingly, classes
have been created in increasing order of abstraction to define the topology of
this neural model: =Neuron.hh=, =Layer.hh= and =NeuralNetwork.hh=. Keeping in
mind the ease of use for the user and allowing the greatest possible
flexibility, classes are also designed for reading data (=Data.hh=) and for
configuring the network (=Custom.hh=).

** Getting Started üöÄ
These instructions will get you a copy of the project up and running on your
local machine for development and testing purposes.

*** Installing üîß
Clone the repository.
#+BEGIN_SRC bash
  git clone https://gitlab.com/david-alvarez-rosa/neural-network.git
#+END_SRC

Compile the code (it may take some time the first time).
#+BEGIN_SRC bash
  cd neural-network/
  make
#+END_SRC

** Running the Algorithm Ô∏è
Once the code is compiled just execute =main=,
#+BEGIN_SRC bash
  ./main
#+END_SRC
and enjoy.

*Note.* The user's configuration together with the reading of the data must
have been modified in the corresponding files prior to its execution.

** Understanding the Code üí°
Paper with implementation coming soon.

** Objectives üéØ
different objectives were had in mind when carrying out this project. They
stand out among them:
- Create a *modular* codebase using object-oriented programming.
- Perform the gradient computation (i.e., partial derivatives of the loss
  function) automatically, accurately (not numerically) and efficiently. The
  algorithm used here is a recursive algorithm proposed by the author, as
  opposed to the one known as /backpropagation/.
- Create code that is *easy* to use by a user with no prior knowledge of Machine
  Learning.
- Allow the user as much *flexibility* as possible: from the choice of activation
  functions (including neuron to neuron) to the number of layers (with their
  corresponding number of neurons), through loss functions.

** Difficulties and Future Work üõ†
The most complex part of the training algorithm of a neural network is the
computation of the gradient of the loss function with respect to the model
parameters (i.e., the computation of each of the partial derivatives). Here we
propose a recursive algorithm adapted exclusively to the topology of this type
of networks, making extensive use of the chain rule for the computation of
derivatives of composite functions (in its multivariate version). However,
thinking about generalization to other more complex models (e.g., convolutional
neural networks or recurrent neural networks), it would be more interesting to
use graphs to represent the models, and, in this way, to be able to make use of
the chain rule iterating through the nodes of the graph in reverse order.

** Authors ‚úíÔ∏è
[[https://david.alvarezrosa.com/][David √Ålvarez Rosa]]
